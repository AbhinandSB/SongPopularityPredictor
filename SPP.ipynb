{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\Anaconda\\lib\\site-packages\\numpy\\.libs\\libopenblas.NOIJJG62EMASZI6NYURL6JBKM4EVBGM7.gfortran-win_amd64.dll\n",
      "D:\\Anaconda\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_song_data= pd.read_csv(\"song_data.csv\")\n",
    "spotify_song_info= pd.read_csv(\"song_info.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_info=spotify_song_info.copy()\n",
    "song_data=spotify_song_data.copy()\n",
    "song_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data.columns[song_data.isnull().any()]\n",
    "song_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data.song_duration_ms= song_data.song_duration_ms.astype(float)\n",
    "song_data.time_signature= song_data.time_signature.astype(float)\n",
    "song_data.audio_mode= song_data.audio_mode.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data[\"popularity\"]= [ 1 if i>=66.5 else 0 for i in song_data.song_popularity ]\n",
    "song_data[\"popularity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#popular songs' data\n",
    "a=song_data[song_data[\"popularity\"]==1]\n",
    "a.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install modules\n",
    "!pip install lyricsgenius\n",
    "!pip install textblob\n",
    "!python -m textblob.download_corpora\n",
    "\n",
    "#import packages\n",
    "from gensim.summarization import keywords\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "nlp.max_length = 10000000\n",
    "import lyricsgenius\n",
    "import pandas as pd\n",
    "from textblob import TextBlob "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_access_token = api_key.your_client_access_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to return lyrics of each song using Genius API\n",
    "def get_lyrics(title, artist):\n",
    "    try:\n",
    "        return genius.search_song(title, artist).lyrics\n",
    "    except:\n",
    "        return 'not found'\n",
    "\n",
    "#Function to return sentiment score of each song\n",
    "def get_lyric_sentiment(lyrics): \n",
    "\tanalysis = TextBlob(lyrics) \n",
    "\treturn analysis.sentiment.polarity\n",
    "\n",
    "def preprocess(text):\n",
    "    # Create Doc object\n",
    "    doc = nlp(text, disable=['ner', 'parser'])\n",
    "    # Generate lemmas\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    # Remove stopwords and non-alphabetic characters\n",
    "    a_lemmas = [lemma for lemma in lemmas \n",
    "            if lemma.isalpha() and lemma not in stopwords]\n",
    "    \n",
    "    return ' '.join(a_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use get_lyrics funcion to get lyrics for every song in dataset\n",
    "lyrics = song_info.apply(lambda row: get_lyrics(row['song_name'], row['artist_name']), axis =1)\n",
    "song_data['lyrics'] = lyrics\n",
    "song_data = song_data.drop(song_data[song_data['lyrics'] == 'not found'].index) #drop rows where lyrics are not found on Genius\n",
    "\n",
    "#Use get_lyric_sentiment to get sentiment score for all the song lyrics\n",
    "sentiment = song_data.apply(lambda row: get_lyric_sentiment(row['lyrics']), axis =1)\n",
    "song_data['sentiment'] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data.to_csv(\"songs.csv\", index = False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(12, 12))\n",
    "mask = np.zeros_like(song_data.corr())\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "sns.heatmap(song_data.corr(), annot=True, linewidths=0.4,linecolor=\"white\", fmt= '.1f',ax=ax,cmap=\"Blues\",mask=mask)\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot(variable):\n",
    "    \n",
    "    var=song_data[variable]\n",
    "    var_value= var.value_counts()\n",
    "    \n",
    "    #visualize\n",
    "    plt.figure(figsize=(9,3))\n",
    "    plt.bar(var_value.index,var_value,color=\"orange\")\n",
    "    plt.xticks(var_value.index,var_value.index.values)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(variable)\n",
    "    plt.show()\n",
    "    print(\"{}:\\n{}\".format(variable,var_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category1 = [\"popularity\",\"key\",\"audio_mode\",\"time_signature\"]\n",
    "for c in category1:\n",
    "    bar_plot(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data[[\"key\",\"popularity\"]].groupby([\"key\"], as_index = False).mean().sort_values(by=\"popularity\",ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data[[\"audio_mode\",\"popularity\"]].groupby([\"audio_mode\"], as_index = False).mean().sort_values(by=\"popularity\",ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_signature vs popularity\n",
    "song_data[[\"time_signature\",\"popularity\"]].groupby([\"time_signature\"], as_index = False).mean().sort_values(by=\"popularity\",ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def detect_outliers(df,features):\n",
    "    outlier_indices = []\n",
    "    \n",
    "    for c in features:\n",
    "        # 1st quartile\n",
    "        Q1 = np.percentile(df[c],25)\n",
    "        # 3rd quartile\n",
    "        Q3 = np.percentile(df[c],75)\n",
    "        # IQR\n",
    "        IQR = Q3 - Q1\n",
    "        # Outlier step\n",
    "        outlier_step = IQR * 1.5\n",
    "        # detect outlier and their indeces\n",
    "        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index #filtre\n",
    "        # store indeces\n",
    "        outlier_indices.extend(outlier_list_col) #The extend() extends the list by adding all items of a list (passed as an argument) to the end.\n",
    "    \n",
    "    outlier_indices = Counter(outlier_indices)\n",
    "    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2) \n",
    "    \n",
    "    return multiple_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data.loc[detect_outliers(song_data,[\"song_popularity\",\"song_duration_ms\",\"danceability\",\"energy\",\"instrumentalness\",\"liveness\",\"loudness\",\"speechiness\",\"audio_valence\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop outliers\n",
    "song_data = song_data.drop(detect_outliers(song_data,[\"song_popularity\",\"song_duration_ms\",\"danceability\",\"energy\",\"instrumentalness\",\"liveness\",\"loudness\",\"speechiness\",\"audio_valence\"]),axis = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data[song_data[\"audio_mode\"].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.factorplot(x = \"key\", y = \"popularity\", data = song_data, kind = \"bar\", size = 6)\n",
    "g.set_ylabels(\"Popularity Probability\")\n",
    "plt.show()\n",
    "plt.title(\"Popularity Based on Key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.factorplot(x = \"audio_mode\", y = \"popularity\", data = song_data, kind = \"bar\", size = 6)\n",
    "g.set_ylabels(\"Popularity Probability\")\n",
    "plt.show()\n",
    "plt.title(\"Popularity Based on Mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.factorplot(x = \"time_signature\", y = \"popularity\", data = song_data, kind = \"bar\", size = 6)\n",
    "g.set_ylabels(\"Popularity Probability\")\n",
    "plt.show()\n",
    "plt.title(\"Popularity Based on Time Signature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(song_data, row = \"audio_mode\", col = \"popularity\", size = 4)\n",
    "g.map(sns.barplot, \"key\", \"acousticness\")\n",
    "g.add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(song_data, row = \"audio_mode\", col = \"popularity\", size = 4)\n",
    "g.map(sns.barplot, \"key\", \"danceability\",color=\"purple\")\n",
    "g.add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(song_data, row = \"audio_mode\", col = \"popularity\", size = 4)\n",
    "g.map(sns.barplot, \"key\", \"instrumentalness\",color=\"green\")\n",
    "g.add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(song_data, row = \"audio_mode\", col = \"popularity\", size = 4)\n",
    "g.map(sns.barplot, \"key\", \"loudness\",color=\"orange\")\n",
    "g.add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(3, 5, figsize=(12, 12))\n",
    "sns.distplot( song_data[\"song_duration_ms\"] , color=\"teal\", ax=axes[0, 0])\n",
    "sns.distplot( song_data[\"instrumentalness\"] , color=\"teal\", ax=axes[0, 1])\n",
    "sns.distplot( song_data[\"acousticness\"] , color=\"teal\", ax=axes[0, 2])\n",
    "sns.distplot( song_data[\"danceability\"] , color=\"teal\", ax=axes[0, 3])\n",
    "sns.distplot( song_data[\"energy\"] , color=\"teal\", ax=axes[0, 4])\n",
    "sns.distplot( song_data[\"song_popularity\"] , color=\"teal\", ax=axes[1, 0])\n",
    "sns.distplot( song_data[\"key\"] , color=\"teal\", ax=axes[1, 1])\n",
    "sns.distplot( song_data[\"liveness\"] , color=\"teal\", ax=axes[1, 2])\n",
    "sns.distplot( song_data[\"loudness\"] , color=\"teal\", ax=axes[1, 3])\n",
    "sns.distplot( song_data[\"audio_mode\"] , color=\"teal\", ax=axes[1, 4])\n",
    "sns.distplot( song_data[\"tempo\"] , color=\"teal\", ax=axes[2, 0])\n",
    "sns.distplot( song_data[\"speechiness\"] , color=\"teal\", ax=axes[2, 1])\n",
    "sns.distplot( song_data[\"time_signature\"] , color=\"teal\", ax=axes[2, 2])\n",
    "sns.distplot( song_data[\"audio_valence\"] , color=\"teal\", ax=axes[2, 3])\n",
    "f.delaxes(axes[2][4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(song_data, col = \"popularity\")\n",
    "g.map(sns.distplot, \"acousticness\", bins = 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(song_data, col = \"popularity\")\n",
    "g.map(sns.distplot, \"danceability\", bins = 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(song_data, col = \"popularity\")\n",
    "g.map(sns.distplot, \"loudness\", bins = 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(song_data, col = \"popularity\")\n",
    "g.map(sns.distplot, \"instrumentalness\", bins = 25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data3=song_data.copy()\n",
    "song_data3[\"song_audio_valence\"]= [ \"Happy\" if i>=0.5 else \"Sad\" for i in song_data.audio_valence ]\n",
    "song_data3[\"song_audio_valence\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data1=song_data3[song_data3[\"song_popularity\"]>66.5]\n",
    "song_data1[\"song_audio_valence\"]= [ \"Happy\" if i>=0.5 else \"Sad\" for i in song_data1.audio_valence ]\n",
    "song_data1[\"song_audio_valence\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data2_new=song_data1[song_data1[\"song_popularity\"]>90]\n",
    "song_data2_new[\"song_audio_valence\"]= [ \"Happy\" if i>=0.5 else \"Sad\" for i in song_data2_new.audio_valence ]\n",
    "song_data2_new[\"song_audio_valence\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data2= song_data2_new[song_data2_new.popularity==1]\n",
    "a=song_data2.iloc[:,1]\n",
    "a.to_numpy()\n",
    "b=song_data2.iloc[:,14]\n",
    "b.to_numpy()\n",
    "plt.figure(figsize=[8,8])\n",
    "markerline, stemlines, baseline = plt.stem(\n",
    "    a, b, linefmt='grey', markerfmt='D', bottom=0.5)\n",
    "markerline.set_markerfacecolor('none')\n",
    "plt.xlabel(\"Popularity\")\n",
    "plt.ylabel(\"Audio Valance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.concat([song_info, song_data1],axis=1)\n",
    "new_data=new_data[new_data[\"song_popularity\"]>90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 500 Playlists Genre\n",
    "plt.figure(figsize=(12,5))\n",
    "new_data= song_info['playlist'].head(500)\n",
    "g = sns.countplot(new_data, palette=\"icefire\")\n",
    "plt.title(\"Top 500 Genres\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_song_data[\"popularity\"]= [ 1 if i>=66.5 else 0 for i in spotify_song_data.song_popularity ]\n",
    "spotify_song_data[\"popularity\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plr = pd.concat([spotify_song_data.popularity,spotify_song_data.song_name],axis=1)\n",
    "data_plr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data['song_name'].value_counts(dropna=False)\n",
    "song_data['song_name'].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk \n",
    "import nltk as nlp\n",
    "\n",
    "nltk.download(\"stopwords\") \n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Song_Name = [ word for word in spotify_song_data.song_name if not word in set(stopwords.words(\"english\"))]\n",
    "lemma = nlp.WordNetLemmatizer()\n",
    "Song_Name = [ lemma.lemmatize(word) for word in Song_Name] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Song_Name_list = []\n",
    "for Song_Name in data_plr.song_name:\n",
    "    Song_Name = re.sub(\"[^a-zA-Z]\",\" \",Song_Name)\n",
    "    Song_Name = Song_Name.lower() \n",
    "    Song_Name = nltk.word_tokenize(Song_Name)\n",
    "    lemma = nlp.WordNetLemmatizer()\n",
    "    Song_Name = [ lemma.lemmatize(word) for word in Song_Name]\n",
    "    Song_Name = \" \".join(Song_Name)\n",
    "    Song_Name_list.append(Song_Name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% bag of words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "max_features = 10\n",
    "count_vectorizer = CountVectorizer(max_features=max_features,stop_words = \"english\")\n",
    "sparce_matrix = count_vectorizer.fit_transform(Song_Name_list).toarray()  \n",
    "print(\"Most frequenctly used {} words: {}\".format(max_features,count_vectorizer.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " a = count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(Song_Name_list,columns=['Names'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist= song_info.artist_name.tolist()\n",
    "name_count = Counter(artist)         \n",
    "most_common_names2 = name_count.most_common(10) \n",
    "most_common_names2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "data_plr['sentiment'] = data_plr['song_name'].map(lambda text: TextBlob(text).sentiment.polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "cut = pd.cut(\n",
    "    data_plr['sentiment'],\n",
    "    [-np.inf, -.01, .01, np.inf],\n",
    "    labels=['negative', 'neutral', 'positive']\n",
    ")\n",
    "data_plr['polarity'] = cut.values\n",
    "data_plr[['polarity','sentiment']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "data= data_plr.polarity\n",
    "g = sns.countplot(data, palette=\"Set3\")\n",
    "plt.title(\"Songs' Polarity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data =pd.concat([song_data,data_plr.sentiment],axis=1)\n",
    "song_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data[\"key\"] = song_data[\"key\"].astype(\"category\")\n",
    "song_data = pd.get_dummies(song_data, columns=[\"key\"])\n",
    "song_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data[\"audio_mode\"] = song_data[\"audio_mode\"].astype(\"category\")\n",
    "song_data = pd.get_dummies(song_data, columns=[\"audio_mode\"])\n",
    "song_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data[\"time_signature\"] = song_data[\"time_signature\"].astype(\"category\")\n",
    "song_data = pd.get_dummies(song_data, columns=[\"time_signature\"])\n",
    "song_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data.drop([\"song_popularity\",\"song_name\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data.columns[song_data.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill nan values\n",
    "song_data['song_duration_ms'] = song_data['song_duration_ms'].fillna(np.mean(song_data['song_duration_ms']))\n",
    "song_data['acousticness'] = song_data['acousticness'].fillna(np.mean(song_data['acousticness']))\n",
    "song_data['danceability'] = song_data['danceability'].fillna(np.mean(song_data['danceability']))\n",
    "song_data['energy'] = song_data['energy'].fillna(np.mean(song_data['energy']))\n",
    "song_data['instrumentalness'] = song_data['instrumentalness'].fillna(np.mean(song_data['instrumentalness']))\n",
    "song_data['liveness'] = song_data['liveness'].fillna(np.mean(song_data['liveness']))\n",
    "song_data['loudness'] = song_data['loudness'].fillna(np.mean(song_data['loudness']))\n",
    "song_data['speechiness'] = song_data['speechiness'].fillna(np.mean(song_data['speechiness']))\n",
    "song_data['tempo'] = song_data['tempo'].fillna(np.mean(song_data['tempo']))\n",
    "song_data['audio_valence'] = song_data['audio_valence'].fillna(np.mean(song_data['audio_valence']))\n",
    "song_data['popularity'] = song_data['popularity'].fillna(np.mean(song_data['popularity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song_data.columns[song_data.isnull().any()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_type(var):\n",
    "    song_data[var] = song_data[var].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column= [\"sentiment\",\"key_0.0\",\"key_1.0\",\"key_2.0\",\"key_3.0\",\"key_4.0\",\"key_5.0\",\"key_6.0\",\"key_7.0\",\"key_8.0\",\"key_9.0\",\"key_10.0\",\"key_11.0\",\"audio_mode_0.0\",\"audio_mode_1.0\",\"time_signature_0.0\",\"time_signature_1.0\",\"time_signature_3.0\",\"time_signature_4.0\",\"time_signature_5.0\"]\n",
    "for i in column:\n",
    "    change_type(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data preparation\n",
    "y = song_data[\"popularity\"].values\n",
    "x_data=song_data.drop([\"popularity\"],axis=1)\n",
    "#normalization\n",
    "x = (x_data - np.min(x_data))/(np.max(x_data)-np.min(x_data)).values#train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=42)\n",
    "x_train = x_train.T\n",
    "x_test = x_test.T\n",
    "y_train = y_train.astype(int).T\n",
    "y_test = y_test.astype(int).T\n",
    "print(\"x_train: \",x_train.shape)\n",
    "print(\"x_test: \",x_test.shape)\n",
    "print(\"y_train: \",y_train.shape)\n",
    "print(\"y_test: \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LR\n",
    "# parameter initialize and sigmoid function\n",
    "def initialize_weights_and_bias(dimension):\n",
    "    w=np.full((dimension,1),0.01)\n",
    "    b=0.0\n",
    "    return w,b\n",
    "\n",
    "def sigmoid(z):\n",
    "    y_head = 1/(1+ np.exp(-z))\n",
    "    return y_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_backward_propagation(w,b,x_train,y_train):\n",
    "    # forward propagation\n",
    "    z = np.dot(w.T,x_train) + b\n",
    "    y_head = sigmoid(z)\n",
    "    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n",
    "    cost = (np.sum(loss))/x_train.shape[1]      \n",
    "    # backward propagation\n",
    "    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))/x_train.shape[1] \n",
    "    derivative_bias = np.sum(y_head-y_train)/x_train.shape[1]                 \n",
    "    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n",
    "    return cost,gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating(learning) parameters\n",
    "def update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n",
    "    cost_list = []\n",
    "    cost_list2 = []\n",
    "    index = []\n",
    "\n",
    "    for i in range(number_of_iterarion):\n",
    "        # make forward and backward propagation and find cost and gradients\n",
    "        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n",
    "        cost_list.append(cost)\n",
    "        # update\n",
    "        w = w - learning_rate * gradients[\"derivative_weight\"]\n",
    "        b = b - learning_rate * gradients[\"derivative_bias\"]\n",
    "        if i % 10 == 0:\n",
    "            cost_list2.append(cost)\n",
    "            index.append(i)\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    parameters = {\"weight\": w,\"bias\": b}\n",
    "    plt.plot(index,cost_list2)\n",
    "    plt.xticks(index,rotation='vertical')\n",
    "    plt.xlabel(\"Number of Iterarion\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.show()\n",
    "    return parameters, gradients, cost_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w,b,x_test):\n",
    "    z = sigmoid(np.dot(w.T,x_test)+b)\n",
    "    Y_prediction = np.zeros((1,x_test.shape[1]))\n",
    "\n",
    "    for i in range(z.shape[1]):\n",
    "        if z[0,i]<= 0.5:\n",
    "            Y_prediction[0,i] = 0\n",
    "        else:\n",
    "            Y_prediction[0,i] = 1\n",
    "\n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n",
    "    # initialize\n",
    "    dimension =  x_train.shape[0]\n",
    "    w,b = initialize_weights_and_bias(dimension)\n",
    "    #update\n",
    "    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n",
    "    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n",
    "    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n",
    "\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n",
    "    \n",
    "logistic_regression(x_train, y_train, x_test, y_test,learning_rate =0.01, num_iterations = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC Curve with logistic regression\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(x_train,y_train)\n",
    "y_pred_prob = logreg.predict_proba(x_test)[:,1]\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr,color=\"red\")\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "k = 10\n",
    "cv_result = cross_val_score(logreg,x_train,y_train,cv=k)\n",
    "cross_val_log=np.sum(cv_result)/k\n",
    "print('Cross_val Scores: ',cv_result)\n",
    "print('Cross_val scores average: ',np.sum(cv_result)/k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GridSearchCV with Logreg\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = {'C': np.logspace(-3, 3, 7), 'penalty': ['l1', 'l2']}\n",
    "logreg = LogisticRegression()\n",
    "logreg_cv = GridSearchCV(logreg,param_grid,cv=3)\n",
    "logreg_cv.fit(x_train,y_train)\n",
    "print(\"Tuned hyperparameters : {}\".format(logreg_cv.best_params_))\n",
    "print(\"Best Accuracy: {}\".format(logreg_cv.best_score_))\n",
    "##numpy.logspace(start, stop, num=50, endpoint=True, base=10.0, dtype=None, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Logistic_score=logreg_cv.best_score_\n",
    "CrossVal_Logistic_score=cross_val_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN prediction\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "x,y = song_data.loc[:,song_data.columns != 'popularity'], song_data.loc[:,'popularity']\n",
    "y=y.astype(int)\n",
    "knn.fit(x,y)\n",
    "prediction = knn.predict(x)\n",
    "print('Prediction: {}'.format(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN Test\n",
    "knn = KNeighborsClassifier(n_neighbors = 1)\n",
    "knn.fit(x_train,y_train)\n",
    "prediction = knn.predict(x_test)\n",
    "print('With KNN (K=3) train accuracy is: ',knn.score(x_train,y_train))\n",
    "print('With KNN (K=3) test accuracy is: ',knn.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neig = np.arange(1, 25)\n",
    "train_accuracy = []\n",
    "test_accuracy = []\n",
    "\n",
    "for i, k in enumerate(neig):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(x_train,y_train)\n",
    "    train_accuracy.append(knn.score(x_train, y_train))\n",
    "    test_accuracy.append(knn.score(x_test, y_test))\n",
    "\n",
    "plt.figure(figsize=[10,6])\n",
    "plt.plot(neig, test_accuracy, label = 'Testing Accuracy')\n",
    "plt.plot(neig, train_accuracy, label = 'Training Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Knn k value VS Accuracy')\n",
    "plt.xlabel('Number of Neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(neig)\n",
    "plt.savefig('graph.png')\n",
    "plt.show()\n",
    "print(\"Best accuracy is {} with K = {}\".format(np.max(test_accuracy),1+test_accuracy.index(np.max(test_accuracy))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "k = 10\n",
    "cv_result = cross_val_score(knn,x_train,y_train,cv=k)  \n",
    "cv_result_knn=np.sum(cv_result)/k\n",
    "print('Cross_val Scores: ',cv_result)\n",
    "print('Cross_val scores average: ',np.sum(cv_result)/k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "grid = {'n_neighbors': np.arange(1,50)}\n",
    "knn = KNeighborsClassifier()\n",
    "knn_cv = GridSearchCV(knn, grid, cv=3) \n",
    "knn_cv.fit(x,y)\n",
    "print(\"Tuned hyperparameter k: {}\".format(knn_cv.best_params_)) \n",
    "print(\"Best accuracy: {}\".format(knn_cv.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KKN_Score= max(test_accuracy)\n",
    "CrossVal_KKN_Score=cv_result_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "from sklearn.svm import SVC\n",
    "svm= SVC(random_state=1)  #kernel='rbf'\n",
    "svm.fit(x_train,y_train)\n",
    "print(\"Train accuracy of svm algo:\",svm.score(x_train,y_train))\n",
    "print(\"Test accuracy of svm algo:\",svm.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "k = 10\n",
    "cv_result = cross_val_score(svm,x_train,y_train,cv=k) \n",
    "cv_result_svm= np.sum(cv_result)/k\n",
    "print('Cross_val Scores: ',cv_result)\n",
    "print('Cross_val scores average: ',np.sum(cv_result)/k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_score= svm.score(x_test,y_test)\n",
    "CrossVal_SVM_score=cv_result_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "steps = [('scalar', StandardScaler()),\n",
    "         ('SVM', SVC())]\n",
    "pipeline = Pipeline(steps)\n",
    "parameters = {'SVM__C':[1, 10, 100],\n",
    "              'SVM__gamma':[0.1, 0.01]}\n",
    "cv = GridSearchCV(pipeline,param_grid=parameters,cv=10)\n",
    "cv.fit(x_train,y_train)\n",
    "y_pred = cv.predict(x_test)\n",
    "\n",
    "print(\"Tuned Model Parameters: {}\".format(cv.best_params_))\n",
    "print(\"Test accuracy: {}\".format(cv.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb= GaussianNB()\n",
    "nb.fit(x_train,y_train)\n",
    "print(\"Train accuracy of naive bayes:\",nb.score(x_train,y_train))\n",
    "print(\"Test accuracy of naive bayes:\",nb.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive_bayes_score=nb.score(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision Tree Classifier\n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,confusion_matrix,f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt= DecisionTreeClassifier()\n",
    "dt.fit(x_train,y_train)\n",
    "y_pred=dt.predict(x_test)\n",
    "DecisionTree_score=dt.score(x_test,y_test)\n",
    "print(\"Train ccuracy of decision tree:\",dt.score(x_train,y_train))\n",
    "print(\"Test accuracy of decision tree:\",dt.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "k =10\n",
    "cv_result = cross_val_score(dt,x_train,y_train,cv=k) # uses R^2 as score \n",
    "print('Cross_val Scores: ',cv_result)\n",
    "print('Cross_val scores average: ',np.sum(cv_result)/k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest Classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf=RandomForestClassifier(n_estimators=150,random_state = 3)\n",
    "rf.fit(x_train,y_train)\n",
    "print(\"Train ccuracy of random forest\",rf.score(x_train,y_train))\n",
    "print(\"Test accuracy of random forest\",rf.score(x_test,y_test))\n",
    "RandomForestClassifier_score=rf.score(x_test,y_test)\n",
    "y_pred=rf.predict(x_test)\n",
    "t_true=y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "k = 10\n",
    "cv_result = cross_val_score(rf,x_train,y_train,cv=k) # uses R^2 as score \n",
    "cv_result_randomforest=np.sum(cv_result)/k\n",
    "print('Cross_val Scores: ',cv_result)\n",
    "print('Cross_val scores average: ',np.sum(cv_result)/k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CrossVal_RandomForestClassifier_score=cv_result_randomforest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(random_state = 4)\n",
    "rf.fit(x_train,y_train)\n",
    "y_pred = rf.predict(x_test)\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print('Confusion matrix: \\n',cm)\n",
    "print('Classification report: \\n',classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cm,annot=True,fmt=\"d\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Voting Classifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "ensemble=VotingClassifier(estimators=[('Random Forest', rf), ('Logistic Regression', logreg)], \n",
    "                       voting='soft', weights=[2,1]).fit(x_train,y_train)\n",
    "print('The train accuracy for Random Forest and Logistic Regression is:',ensemble.score(x_train,y_train))\n",
    "print('The test accuracy for Random Forest and Logistic Regression is:',ensemble.score(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "k = 5\n",
    "cv_result = cross_val_score(ensemble,x_train,y_train,cv=k) # uses R^2 as score \n",
    "print('Cross_val Scores: ',cv_result)\n",
    "print('Cross_val scores average: ',np.sum(cv_result)/k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot feature importance manually\n",
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# fit model no training data\n",
    "model = XGBClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "# feature importance\n",
    "print(model.feature_importances_)\n",
    "# plot\n",
    "#pyplot.bar(range(len(model.feature_importances_)), model.feature_importances_)\n",
    "#pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import loadtxt\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from matplotlib import pyplot\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# plot feature importance\n",
    "ax = plot_importance(model)\n",
    "fig = ax.figure\n",
    "fig.set_size_inches(10, 7)\n",
    "#plot_importance(model)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import sort\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "model = XGBClassifier()\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "predictions = [round(value) for value in y_pred]\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "thresholds = sort(model.feature_importances_) # Fit model using each importance as a threshold\n",
    "\n",
    "for thresh in thresholds:\n",
    "    # select features using threshold\n",
    "    selection = SelectFromModel(model, threshold=thresh, prefit=True) # if prefit=True, you should call transform directly.\n",
    "    select_X_train = selection.transform(x_train)\n",
    "    # train model \n",
    "    selection_model = XGBClassifier()\n",
    "    selection_model.fit(select_X_train, y_train)\n",
    "    # evaluate model\n",
    "    select_X_test = selection.transform(x_test)\n",
    "    y_pred = selection_model.predict(select_X_test)\n",
    "    predictions = [round(value) for value in y_pred]\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performances=pd.DataFrame({'Model':['RandomForestClassifier','SVM','DesicionTreeClassifier','K-NearestNeighbors','LogisticRegession','NaiveBayes','Ridge','Lasso'],\n",
    "                                 'Accuracy':[RandomForestClassifier_score,SVM_score,DecisionTree_score,KKN_Score,Logistic_score,Naive_bayes_score,Ridge_score,Lasso_Score]})\n",
    "model_performances.sort_values(by = \"Accuracy\",ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list= list(model_performances['Model'].unique())\n",
    "accuracy_list= list(model_performances['Accuracy'].sort_values(ascending=False))\n",
    "f,ax = plt.subplots(figsize = (4,6))\n",
    "sns.barplot(x=accuracy_list,y=model_list,color='green',alpha = 0.5)\n",
    "ax.set(xlabel='Test Accuracies', ylabel='Models')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['RF','SVM', 'K-NN', 'LR']\n",
    "accuracy = [0.897531,0.885054, 0.789488,0.710711]\n",
    "validation = [0.878750, 0.875631, 0.714693, 0.710711]\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "rects1 = ax.bar(x - width/2,accuracy, width, label='Accuracy')\n",
    "rects2 = ax.bar(x + width/2, validation, width, label='Validation')\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 2),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "        \n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
